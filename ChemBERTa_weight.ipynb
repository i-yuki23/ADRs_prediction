{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLB3I4FKZ5Lr"
      },
      "source": [
        "# Fine-tuning ChemBERTa (and friends) for multi-label text classification\n",
        "\n",
        "In this notebook, we are going to fine-tune BERT to predict one or more labels for a given piece of text. Note that this notebook illustrates how to fine-tune a bert-base-uncased model, but you can also fine-tune a RoBERTa, DeBERTa, DistilBERT, CANINE, ... checkpoint in the same way. \n",
        "\n",
        "All of those work in the same way: they add a linear layer on top of the base model, which is used to produce a tensor of shape (batch_size, num_labels), indicating the unnormalized scores for a number of labels for every example in the batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIH9NP0MZ6-O"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "Next, let's download a multi-label text classification dataset from the [hub](https://huggingface.co/).\n",
        "\n",
        "At the time of writing, I picked a random one as follows:   \n",
        "\n",
        "* first, go to the \"datasets\" tab on huggingface.co\n",
        "* next, select the \"multi-label-classification\" tag on the left as well as the the \"1k<10k\" tag (fo find a relatively small dataset).\n",
        "\n",
        "Note that you can also easily load your local data (i.e. csv files, txt files, Parquet files, JSON, ...) as explained [here](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "9a1aa9f2cc29473f9f8e5459d2641e76",
            "308fa6a7348140ec981a8d6c7d31f346",
            "8bc92587e35443488445e7521fbd0a13",
            "091f8220f33241f288faa0612853585f",
            "1045bb16e3694410898a73cf1b848917",
            "cb95e545fbdd4e99903bf634df694c9f",
            "5080d322a8034924b652b379c04667ed",
            "8b1899a0c4b144d7a5e6599f8afb8b65",
            "6111a73e684a47769bda7183a836ee91",
            "cd3570ddf67541d7818d97e236c54e54",
            "bbba60f793c14100934a268063f63d26"
          ]
        },
        "id": "sd1LiXGjZ420",
        "outputId": "1b5783cd-0e4e-4c92-c67c-57b9288f2381"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "import os\n",
        "\n",
        "data_dir = \"./data/processed_data/scaffold/\"\n",
        "# Load the datasets\n",
        "train_dataset = load_dataset('csv', data_files=os.path.join(data_dir, \"train/metadata.csv\"))['train']\n",
        "valid_dataset = load_dataset('csv', data_files=os.path.join(data_dir, \"validation/metadata.csv\"))['train']\n",
        "test_dataset = load_dataset('csv', data_files=os.path.join(data_dir, \"test/metadata.csv\"))['train']\n",
        "\n",
        "# Create a DatasetDict\n",
        "datasets = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'valid': valid_dataset,\n",
        "    'test': test_dataset\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCL02vQgxYTO"
      },
      "source": [
        "As we can see, the dataset contains 3 splits: one for training, one for validation and one for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgS0wMWExcqP"
      },
      "source": [
        "Let's check the first example of the training split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unjuTtKUjZI3",
        "outputId": "6f1e5051-8272-40f9-ced8-ba17a105e904"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Unnamed: 0': 1,\n",
              " 'smiles': 'O=C1CN(N=CC2:C:C:C(C3:C:C:C([N+](=O)[O-]):C:C:3):O:2)C([O-])=N1',\n",
              " 'Hepatobiliary disorders': 1,\n",
              " 'Metabolism and nutrition disorders': 1,\n",
              " 'Product issues': 0,\n",
              " 'Eye disorders': 1,\n",
              " 'Investigations': 1,\n",
              " 'Musculoskeletal and connective tissue disorders': 1,\n",
              " 'Gastrointestinal disorders': 1,\n",
              " 'Social circumstances': 0,\n",
              " 'Immune system disorders': 1,\n",
              " 'Reproductive system and breast disorders': 0,\n",
              " 'Neoplasms benign, malignant and unspecified (incl cysts and polyps)': 1,\n",
              " 'General disorders and administration site conditions': 1,\n",
              " 'Endocrine disorders': 0,\n",
              " 'Surgical and medical procedures': 0,\n",
              " 'Vascular disorders': 1,\n",
              " 'Blood and lymphatic system disorders': 1,\n",
              " 'Skin and subcutaneous tissue disorders': 1,\n",
              " 'Congenital, familial and genetic disorders': 0,\n",
              " 'Infections and infestations': 0,\n",
              " 'Respiratory, thoracic and mediastinal disorders': 1,\n",
              " 'Psychiatric disorders': 1,\n",
              " 'Renal and urinary disorders': 1,\n",
              " 'Pregnancy, puerperium and perinatal conditions': 0,\n",
              " 'Ear and labyrinth disorders': 0,\n",
              " 'Cardiac disorders': 1,\n",
              " 'Nervous system disorders': 1,\n",
              " 'Injury, poisoning and procedural complications': 1}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = datasets['train'][1]\n",
        "example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DV0Rtetxgd4"
      },
      "source": [
        "The dataset consists of smiles, labeled with one or more ADRs. \n",
        "\n",
        "Let's create a list that contains the labels, as well as 2 dictionaries that map labels to integers and back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5vZhQpvkE8s",
        "outputId": "5d513b30-f209-492f-c6ab-245d64a67d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hepatobiliary disorders', 'Metabolism and nutrition disorders', 'Product issues', 'Eye disorders', 'Investigations', 'Musculoskeletal and connective tissue disorders', 'Gastrointestinal disorders', 'Social circumstances', 'Immune system disorders', 'Reproductive system and breast disorders', 'Neoplasms benign, malignant and unspecified (incl cysts and polyps)', 'General disorders and administration site conditions', 'Endocrine disorders', 'Surgical and medical procedures', 'Vascular disorders', 'Blood and lymphatic system disorders', 'Skin and subcutaneous tissue disorders', 'Congenital, familial and genetic disorders', 'Infections and infestations', 'Respiratory, thoracic and mediastinal disorders', 'Psychiatric disorders', 'Renal and urinary disorders', 'Pregnancy, puerperium and perinatal conditions', 'Ear and labyrinth disorders', 'Cardiac disorders', 'Nervous system disorders', 'Injury, poisoning and procedural complications']\n",
            "{0: 'Hepatobiliary disorders', 1: 'Metabolism and nutrition disorders', 2: 'Product issues', 3: 'Eye disorders', 4: 'Investigations', 5: 'Musculoskeletal and connective tissue disorders', 6: 'Gastrointestinal disorders', 7: 'Social circumstances', 8: 'Immune system disorders', 9: 'Reproductive system and breast disorders', 10: 'Neoplasms benign, malignant and unspecified (incl cysts and polyps)', 11: 'General disorders and administration site conditions', 12: 'Endocrine disorders', 13: 'Surgical and medical procedures', 14: 'Vascular disorders', 15: 'Blood and lymphatic system disorders', 16: 'Skin and subcutaneous tissue disorders', 17: 'Congenital, familial and genetic disorders', 18: 'Infections and infestations', 19: 'Respiratory, thoracic and mediastinal disorders', 20: 'Psychiatric disorders', 21: 'Renal and urinary disorders', 22: 'Pregnancy, puerperium and perinatal conditions', 23: 'Ear and labyrinth disorders', 24: 'Cardiac disorders', 25: 'Nervous system disorders', 26: 'Injury, poisoning and procedural complications'}\n",
            "{'Hepatobiliary disorders': 0, 'Metabolism and nutrition disorders': 1, 'Product issues': 2, 'Eye disorders': 3, 'Investigations': 4, 'Musculoskeletal and connective tissue disorders': 5, 'Gastrointestinal disorders': 6, 'Social circumstances': 7, 'Immune system disorders': 8, 'Reproductive system and breast disorders': 9, 'Neoplasms benign, malignant and unspecified (incl cysts and polyps)': 10, 'General disorders and administration site conditions': 11, 'Endocrine disorders': 12, 'Surgical and medical procedures': 13, 'Vascular disorders': 14, 'Blood and lymphatic system disorders': 15, 'Skin and subcutaneous tissue disorders': 16, 'Congenital, familial and genetic disorders': 17, 'Infections and infestations': 18, 'Respiratory, thoracic and mediastinal disorders': 19, 'Psychiatric disorders': 20, 'Renal and urinary disorders': 21, 'Pregnancy, puerperium and perinatal conditions': 22, 'Ear and labyrinth disorders': 23, 'Cardiac disorders': 24, 'Nervous system disorders': 25, 'Injury, poisoning and procedural complications': 26}\n"
          ]
        }
      ],
      "source": [
        "labels = [label for label in datasets['train'].features.keys() if label not in ['Unnamed: 0', 'smiles']]\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "print(labels)\n",
        "print(id2label)\n",
        "print(label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ3Teyjmank2"
      },
      "source": [
        "## Preprocess data\n",
        "\n",
        "As models like BERT don't expect text as direct input, but rather `input_ids`, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub.\n",
        "\n",
        "What's a bit tricky is that we also need to provide labels to the model. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' `BCEWithLogitsLoss` (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AFWlSsbZaRLc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "pretrained_path = \"seyonec/PubChem10M_SMILES_BPE_450k\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n",
        "\n",
        "def preprocess_data(examples):\n",
        "  # take a batch of texts\n",
        "  smiles = examples[\"smiles\"]\n",
        "  # encode them\n",
        "  encoding = tokenizer(smiles, padding=\"max_length\", truncation=True, max_length=300)\n",
        "  # add labels\n",
        "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
        "  # create numpy array of shape (batch_size, num_labels)\n",
        "  labels_matrix = np.zeros((len(smiles), len(labels)))\n",
        "  # fill numpy array\n",
        "  for idx, label in enumerate(labels):\n",
        "    labels_matrix[:, idx] = labels_batch[label]\n",
        "\n",
        "  encoding[\"labels\"] = labels_matrix.tolist()\n",
        "  \n",
        "  return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4ENBTdulBEI",
        "outputId": "02554a1f-4961-461a-bf29-555b8debeabf"
      },
      "outputs": [],
      "source": [
        "encoded_dataset = datasets.map(preprocess_data, batched=True, remove_columns=datasets['train'].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1143\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 142\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 142\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0enAb0W9o25W",
        "outputId": "55bc5ba6-d169-49c6-f562-bb7ea4143866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [0, 262, 263, 51, 13, 50, 12, 262, 12, 51, 13, 291, 12, 39, 12, 39, 287, 51, 13, 39, 21, 30, 39, 12, 45, 13, 30, 39, 12, 39, 263, 51, 13, 301, 12, 51, 13, 298, 13, 30, 39, 12, 45, 13, 30, 39, 12, 39, 263, 51, 13, 301, 12, 51, 13, 298, 13, 30, 39, 30, 21, 45, 13, 39, 21, 30, 39, 12, 45, 13, 30, 39, 12, 39, 263, 51, 13, 301, 12, 51, 13, 298, 13, 30, 39, 12, 45, 13, 30, 39, 12, 39, 263, 51, 13, 301, 12, 51, 13, 298, 13, 30, 39, 30, 21, 45, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}\n",
            "300\n",
            "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "example = encoded_dataset['train'][0]\n",
        "print(example)\n",
        "print(len(example['input_ids']))\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "D0McCtJ8HRJY",
        "outputId": "82fb0336-51a3-40ad-ebc0-65eeb7cf4b6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<s>CC(=O)N(CC(O)CN(C(C)=O)C1:C(I):C(C(=O)NCC(O)CO):C(I):C(C(=O)NCC(O)CO):C:1I)C1:C(I):C(C(=O)NCC(O)CO):C(I):C(C(=O)NCC(O)CO):C:1I</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdIvj6WjHeZQ",
        "outputId": "418c14d2-cca3-44e9-d0a2-6ad4ca7a007c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 0.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0,\n",
              " 1.0]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Dx95t2o6N9",
        "outputId": "3ce6c923-0b45-4743-bc4b-7771d088b03e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Metabolism and nutrition disorders',\n",
              " 'Eye disorders',\n",
              " 'Investigations',\n",
              " 'Musculoskeletal and connective tissue disorders',\n",
              " 'Gastrointestinal disorders',\n",
              " 'Immune system disorders',\n",
              " 'General disorders and administration site conditions',\n",
              " 'Vascular disorders',\n",
              " 'Blood and lymphatic system disorders',\n",
              " 'Skin and subcutaneous tissue disorders',\n",
              " 'Infections and infestations',\n",
              " 'Respiratory, thoracic and mediastinal disorders',\n",
              " 'Psychiatric disorders',\n",
              " 'Renal and urinary disorders',\n",
              " 'Ear and labyrinth disorders',\n",
              " 'Cardiac disorders',\n",
              " 'Nervous system disorders',\n",
              " 'Injury, poisoning and procedural complications']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgpKXDfvKBxn"
      },
      "source": [
        "Finally, we set the format of our data to PyTorch tensors. This will turn the training, validation and test sets into standard PyTorch [datasets](https://pytorch.org/docs/stable/data.html). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Lk6Cq9duKBkA"
      },
      "outputs": [],
      "source": [
        "encoded_dataset.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5qSmCgWefWs"
      },
      "source": [
        "## Define model\n",
        "\n",
        "Here we define a model that includes a pre-trained base (i.e. the weights from bert-base-uncased) are loaded, with a random initialized classification head (linear layer) on top. One should fine-tune this head, together with the pre-trained base on a labeled dataset.\n",
        "\n",
        "This is also printed by the warning.\n",
        "\n",
        "We set the `problem_type` to be \"multi_label_classification\", as this will make sure the appropriate loss function is used (namely [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)). We also make sure the output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class WeightedBCEWithLogitsLoss(nn.Module):\n",
        "    def __init__(self, weights):\n",
        "        super(WeightedBCEWithLogitsLoss, self).__init__()\n",
        "        self.weights = weights\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        loss = loss_fn(logits, targets)\n",
        "        weighted_loss = loss * self.weights\n",
        "        return weighted_loss.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XPL1Z_RegBF",
        "outputId": "22994300-8c93-421e-faa4-678d6cc14aab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ito/anaconda3/envs/project/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/PubChem10M_SMILES_BPE_450k and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(pretrained_path, \n",
        "                                                           problem_type=\"multi_label_classification\", \n",
        "                                                           num_labels=len(labels),\n",
        "                                                           id2label=id2label,\n",
        "                                                           label2id=label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjJGEXShp7te"
      },
      "source": [
        "## Train the model!\n",
        "\n",
        "We are going to train the model using HuggingFace's Trainer API. This requires us to define 2 things: \n",
        "\n",
        "* `TrainingArguments`, which specify training hyperparameters. All options can be found in the [docs](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). Below, we for example specify that we want to evaluate after every epoch of training, we would like to save the model every epoch, we set the learning rate, the batch size to use for training/evaluation, how many epochs to train for, and so on.\n",
        "* a `Trainer` object (docs can be found [here](https://huggingface.co/transformers/main_classes/trainer.html#id1))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K5a8_vIKqr7P"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "metric_name = \"f1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "dR2GmpvDqbuZ"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"ChemBERTa_weighted\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    #push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_v2fPFFJ3-v"
      },
      "source": [
        "We are also going to compute metrics while training. For this, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "797b2WHJqUgZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "    \n",
        "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
        "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
        "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "    # finally, compute metrics\n",
        "    y_true = labels\n",
        "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
        "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    # return as dictionary\n",
        "    metrics = {'f1': f1_micro_average,\n",
        "               'roc_auc': roc_auc,\n",
        "               'accuracy': accuracy}\n",
        "    return metrics\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, \n",
        "            tuple) else p.predictions\n",
        "    result = multi_label_metrics(\n",
        "        predictions=preds, \n",
        "        labels=p.label_ids)\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxNo4_TsvzDm"
      },
      "source": [
        "Let's verify a batch as well as a forward pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IlOgGiojuWwG",
        "outputId": "cd0b2c99-b520-468d-8ffc-c36211b7820a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_dataset['train'][0]['labels'].type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y41Kre_jvD7x",
        "outputId": "b6ca888b-6371-40fb-ab83-3dc24d28320a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  0, 262, 263,  51,  13,  50,  12, 262,  12,  51,  13, 291,  12,  39,\n",
              "         12,  39, 287,  51,  13,  39,  21,  30,  39,  12,  45,  13,  30,  39,\n",
              "         12,  39, 263,  51,  13, 301,  12,  51,  13, 298,  13,  30,  39,  12,\n",
              "         45,  13,  30,  39,  12,  39, 263,  51,  13, 301,  12,  51,  13, 298,\n",
              "         13,  30,  39,  30,  21,  45,  13,  39,  21,  30,  39,  12,  45,  13,\n",
              "         30,  39,  12,  39, 263,  51,  13, 301,  12,  51,  13, 298,  13,  30,\n",
              "         39,  12,  45,  13,  30,  39,  12,  39, 263,  51,  13, 301,  12,  51,\n",
              "         13, 298,  13,  30,  39,  30,  21,  45,   2,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_dataset['train']['input_ids'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxWcnZ8ku12V",
        "outputId": "26522911-c3cd-466a-ae2d-d81d23003c23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=tensor(0.2913, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.0543,  1.0951, -3.1105,  0.3504,  1.8950,  1.0990,  2.4127, -1.4540,\n",
              "          1.5247,  0.1080, -1.0859,  2.0167, -1.2291, -1.5817,  1.8846,  0.6076,\n",
              "          2.6498, -1.2971,  1.1798,  1.2497,  0.6378,  0.7856, -2.0823, -0.0781,\n",
              "          0.9122,  2.4336,  0.9523]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#forward pass\n",
        "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-X2brZcv0X6"
      },
      "source": [
        "Let's start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.35.2\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.9948,  0.4524, 59.1579,  0.6422,  0.2505,  0.4505,  0.1151,  5.0476,\n",
            "         0.4059,  1.0159,  2.8746,  0.1097,  3.6653,  5.8443,  0.3033,  0.6662,\n",
            "         0.0896,  4.4689,  0.4252,  0.3527,  0.4323,  0.6099, 10.7835,  1.2544,\n",
            "         0.4673,  0.1022,  0.5488], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "from transformers import Trainer\n",
        "\n",
        "weights = torch.load(\"./data/processed_data/train_class_dist_rate.pt\")\n",
        "print(weights)\n",
        "\n",
        "# Initialize custom loss function with the calculated weights\n",
        "loss_func = WeightedBCEWithLogitsLoss(weights=weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, model, args, loss_func=None, **kwargs):\n",
        "        # Store the custom loss function\n",
        "        self.loss_func = loss_func\n",
        "\n",
        "        # Call the parent class's constructor without the custom loss function\n",
        "        super().__init__(model=model, args=args, **kwargs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get('labels')\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Use custom loss function if provided\n",
        "        if self.loss_func:\n",
        "            loss = self.loss_func(logits, labels.float())\n",
        "        else:\n",
        "            # Default to the standard loss calculation\n",
        "            loss = outputs.loss if 'loss' in outputs else None\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "chq_3nUz73ib"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "trainer = CustomTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"valid\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    loss_func=loss_func\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KXmFds8js6P8",
        "outputId": "66ebb2ab-f93f-48aa-a4dc-36f55f2f8559"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='355' max='355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [355/355 05:58, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Roc Auc</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.973603</td>\n",
              "      <td>0.831231</td>\n",
              "      <td>0.749575</td>\n",
              "      <td>0.021127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.981648</td>\n",
              "      <td>0.831887</td>\n",
              "      <td>0.754578</td>\n",
              "      <td>0.021127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.000904</td>\n",
              "      <td>0.829229</td>\n",
              "      <td>0.751090</td>\n",
              "      <td>0.021127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=355, training_loss=0.8210913429797535, metrics={'train_runtime': 359.1246, 'train_samples_per_second': 15.914, 'train_steps_per_second': 0.989, 'total_flos': 440754008666400.0, 'train_loss': 0.8210913429797535, 'epoch': 4.97})"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiloh9eMK91o"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "After training, we evaluate our model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "cMlebJ83LRYG",
        "outputId": "b18102e7-2198-4beb-c874-d39636f740ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5/36 00:00 < 00:01, 17.13 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.9816477756341142,\n",
              " 'eval_f1': 0.8318869828456106,\n",
              " 'eval_roc_auc': 0.754578167333307,\n",
              " 'eval_accuracy': 0.02112676056338028,\n",
              " 'eval_runtime': 2.2533,\n",
              " 'eval_samples_per_second': 63.02,\n",
              " 'eval_steps_per_second': 15.977,\n",
              " 'epoch': 4.97}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'path_to_your_project/ChemBERTa/checkpoint-360' with the actual path\n",
        "model_path = './ChemBERTa_weighted/checkpoint-286'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset = pd.read_csv(os.path.join(data_dir, \"test/metadata.csv\"))\n",
        "test_smiles = test_dataset[\"smiles\"].values\n",
        "test_ground_truth = test_dataset.drop([\"smiles\", \"Unnamed: 0\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_ground_truth_values = test_ground_truth.values\n",
        "test_smiles.shape\n",
        "test_smiles_list = test_smiles.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_inputs = tokenizer(test_smiles_list, padding=\"max_length\", truncation=True, max_length=300, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Assuming you are using a PyTorch model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokenized_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.4720, 0.6340, 0.0036, 0.6293, 0.8017, 0.6970, 0.8754, 0.1200, 0.7078,\n",
              "        0.4500, 0.2256, 0.8912, 0.1557, 0.0874, 0.7558, 0.5509, 0.9042, 0.1179,\n",
              "        0.6518, 0.7003, 0.6899, 0.6468, 0.0261, 0.5428, 0.6183, 0.8828, 0.6678])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probabilities = torch.sigmoid(outputs.logits)\n",
        "probabilities[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
              "        1, 1, 1], dtype=torch.int32)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thredshold = 0.5\n",
        "predictions = (probabilities > thredshold).int()\n",
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "Average Accuracy: 0.7767344809598329\n",
            "Average Precision: 0.6074995658309068\n",
            "Average Recall: 0.6921425327089815\n",
            "Average F1 Score: 0.6222410258944913\n",
            "Average AUC: 0.6345645218304565\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Convert your tensors to numpy arrays if they aren't already\n",
        "predictions_np = predictions.numpy()\n",
        "true_labels_np = test_ground_truth_values  # Replace true_labels with your actual labels\n",
        "probabilities_np = probabilities.numpy()\n",
        "\n",
        "# Initialize lists to store metrics for each label\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "AUC_scores = []\n",
        "\n",
        "# Calculate metrics for each label\n",
        "for label in range(predictions_np.shape[1]):\n",
        "    print(label)\n",
        "    accuracies.append(accuracy_score(true_labels_np[:, label], predictions_np[:, label]))\n",
        "    precisions.append(precision_score(true_labels_np[:, label], predictions_np[:, label], zero_division=0))\n",
        "    recalls.append(recall_score(true_labels_np[:, label], predictions_np[:, label], zero_division=0))\n",
        "    f1_scores.append(f1_score(true_labels_np[:, label], predictions_np[:, label], zero_division=0))\n",
        "    if (true_labels_np[:, label].sum() != 0):\n",
        "        AUC_scores.append(roc_auc_score(true_labels_np[:, label], probabilities_np[:, label]))\n",
        "\n",
        "# Calculate average of each metric\n",
        "average_accuracy = np.mean(accuracies)\n",
        "average_precision = np.mean(precisions)\n",
        "average_recall = np.mean(recalls)\n",
        "average_f1_score = np.mean(f1_scores)\n",
        "average_AUC = np.mean(AUC_scores)\n",
        "\n",
        "print(f\"Average Accuracy: {average_accuracy}\")\n",
        "print(f\"Average Precision: {average_precision}\")\n",
        "print(f\"Average Recall: {average_recall}\")\n",
        "print(f\"Average F1 Score: {average_f1_score}\")\n",
        "print(f\"Average AUC: {average_AUC}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.716577540106952, 0.8215767634854771, 0.0, 0.75, 0.916030534351145, 0.859437751004016, 0.982078853046595, 0.0, 0.8685258964143426, 0.631578947368421, 0.0, 0.9710144927536231, 0.052631578947368425, 0.0, 0.8818897637795275, 0.8355555555555555, 0.982078853046595, 0.0, 0.8215767634854771, 0.8455284552845529, 0.880952380952381, 0.8067226890756304, 0.0, 0.5245901639344263, 0.8284518828451882, 0.9747292418772564, 0.8489795918367347]\n"
          ]
        }
      ],
      "source": [
        "print(f1_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[0.7024390243902439, 0.8166666666666667, 0.0, 0.7555555555555554, 0.916030534351145, 0.859437751004016, 0.982078853046595, 0.0, 0.873015873015873, 0.7103825136612022, 0.0, 0.9710144927536231, 0.052631578947368425, 0.0, 0.8774703557312252, 0.8220338983050849, 0.982078853046595, 0.0, 0.8264462809917354, 0.8455284552845529, 0.880952380952381, 0.8215767634854771, 0.0, 0.4827586206896552, 0.825, 0.9747292418772564, 0.8455284552845529]\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMiblo1Ci0GTlAbbA5wB3mn",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Fine-tuning BERT (and friends) for multi-label text classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "091f8220f33241f288faa0612853585f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6111a73e684a47769bda7183a836ee91",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b1899a0c4b144d7a5e6599f8afb8b65",
            "value": 3
          }
        },
        "1045bb16e3694410898a73cf1b848917": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbba60f793c14100934a268063f63d26",
            "placeholder": "",
            "style": "IPY_MODEL_cd3570ddf67541d7818d97e236c54e54",
            "value": " 3/3 [00:00&lt;00:00, 75.93it/s]"
          }
        },
        "308fa6a7348140ec981a8d6c7d31f346": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5080d322a8034924b652b379c04667ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6111a73e684a47769bda7183a836ee91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b1899a0c4b144d7a5e6599f8afb8b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8bc92587e35443488445e7521fbd0a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5080d322a8034924b652b379c04667ed",
            "placeholder": "",
            "style": "IPY_MODEL_cb95e545fbdd4e99903bf634df694c9f",
            "value": "100%"
          }
        },
        "9a1aa9f2cc29473f9f8e5459d2641e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bc92587e35443488445e7521fbd0a13",
              "IPY_MODEL_091f8220f33241f288faa0612853585f",
              "IPY_MODEL_1045bb16e3694410898a73cf1b848917"
            ],
            "layout": "IPY_MODEL_308fa6a7348140ec981a8d6c7d31f346"
          }
        },
        "bbba60f793c14100934a268063f63d26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb95e545fbdd4e99903bf634df694c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd3570ddf67541d7818d97e236c54e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
